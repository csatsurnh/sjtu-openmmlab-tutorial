{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "127dd72e-7ed6-4e6b-8864-be0b479ccf2e",
   "metadata": {},
   "source": [
    "# Train Image Classifiers\n",
    "\n",
    "In this notebook we will train an image classifier that classify fruit images, using MMClassificaiton."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22298ae9-0024-4e86-a903-4981053f5424",
   "metadata": {},
   "source": [
    "## Prepare a Dataset\n",
    "\n",
    "We have already prepared a dataset.\n",
    "\n",
    "Credit to Zihao: https://github.com/TommyZihao/MMClassification_Tutorials\n",
    "\n",
    "To download and extract the dataset, in command line:\n",
    "```\n",
    "curl -O https://zihao-openmmlab.obs.myhuaweicloud.com/20220716-mmclassification/dataset/fruit30/fruit30_split.zip\n",
    "unzip -d data fruit30_split.zip\n",
    "```\n",
    "\n",
    "The dataset should be categorized by folders, for MMClassification to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd9fc5f-d7a7-4bf4-8c43-ab7156e81831",
   "metadata": {},
   "source": [
    "## Prepare a Config and Checkpoint File\n",
    "\n",
    "For speed consideration, we use a lightweight neural network, MobileNetV2.\n",
    "\n",
    "we use mim to download the config file and checkpoint file.\n",
    "\n",
    "```\n",
    "mim download mmcls --config mobilenet-v2_8xb32_in1k --dest .\n",
    "mv mobilenet-v2_8xb32_in1k.py mobilenet-v2_fruit.py\n",
    "```\n",
    "\n",
    "If you prefer to play with other models, navigate to [MMClassification model zoo](https://mmclassification.readthedocs.io/en/latest/model_zoo.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ca0906-acf9-4096-878a-7504e0d0622a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mim download mmcls --config mobilenet-v2_8xb32_in1k --dest .\n",
    "!mv mobilenet-v2_8xb32_in1k.py mobilenet-v2_fruit.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e6ada9-8978-495c-86e2-a965799ddbb9",
   "metadata": {},
   "source": [
    "## Modify the Config File\n",
    "\n",
    "1. Remove some intermediate item for clean: `dataset_type`, `img_norm_cfg`, `train_pipeline`, `test_pipeline`\n",
    "1. Modify model\n",
    "    1. number of class: from 1000 to 30\n",
    "    2. pretrain weights: from None to the downloaded checkpoint file, as we finetune the model instead of training from scratch\n",
    "1. Data: for train/val/test \n",
    "    1. `type`: `ImageNet` -> `CustomDataset`\n",
    "    2. `prefix`, which is the root path to images: modify to `\"data/fruit30_split/train\"` or `\"data/fruit30_split/val\"`\n",
    "    3. `ann_file`, use folder name as class name: modify to `None`\n",
    "1. Runner and Optimizer\n",
    "    1. number of training epochs: `runner.max_epochs`\n",
    "    1. learning rates: `optimizer.lr`, usually divided by 8 due to linear scaling rules.\n",
    "1. Misc\n",
    "    1. Decrease `log_confg.interval` for small computation power\n",
    "    1. Increase `checkpoint_config.interval` to avoid saving too many checkpoint, to same time and disk space\n",
    "1. Further parameter tuning you may try\n",
    "    1. learning rates: Decrease `optimizer.lr` for finetuning \n",
    "    1. configure learning scheduler to decrease learning when loss saturates. Moreover, by setting `by_epoch=False`, we decrease learning rate by iteration instead of by epoches.\n",
    "    1. Monitor loss decrease and re-tune\n",
    "    1. More available lr_schedulers are available in [mmcv](https://github.com/open-mmlab/mmcv/blob/master/mmcv/runner/hooks/lr_updater.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8071497-dd81-4121-b2b0-91df11d41734",
   "metadata": {},
   "source": [
    "## Launch Training\n",
    "\n",
    "In command line\n",
    "\n",
    "```\n",
    "mim train mmcls mobilenet-v2_fruit.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da09fe8-4d5c-43d5-9456-0e7bcf8e740c",
   "metadata": {},
   "source": [
    "## Understand Logs\n",
    "\n",
    "\n",
    "The log is long but mainly contains the following parts:\n",
    "\n",
    "1. Toolbox information\n",
    "2. Dumped Config files\n",
    "3. Model Initialization Logs\n",
    "    1. Check `mmcls - INFO - load checkpoint from local path: mobilenet_v2_batch256_imagenet_20200708-3b2dc3af.pth`, which means pretrained weights are loaded correctly.\n",
    "4. Information on Hooks: we don't configure this explicitly in this tutorial, so ignore that\n",
    "5. Training progress\n",
    "    1. Training logs: including current learning, training loss, time consumption, memory occupation\n",
    "    2. Validation logs: Accuracy on validation set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515824b8-39f0-415f-8636-76f2cb7423f7",
   "metadata": {},
   "source": [
    "## Test the Model\n",
    "\n",
    "The trained model (checkpoint file) is usually saved under `work_dirs/{experiment_name}/latest.pth`. \n",
    "We can load it to test with a new image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c30d1a-25d6-4fac-8aee-21003c6c8109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmcls.apis import init_model, inference_model\n",
    "\n",
    "model = init_model('mobilenet-v2_fruit.py', 'work_dirs/mobilenet-v2_fruit/latest.pth')\n",
    "result = inference_model(model, 'banana.png')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b36d5d6-bacf-4cf7-86f4-0b3b54230a73",
   "metadata": {},
   "source": [
    "## PyTorch codes under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f17de7-87ab-4069-9961-dbdfb6615e26",
   "metadata": {},
   "source": [
    "### Runner\n",
    "\n",
    "Runner construct the framework of training.\n",
    "\n",
    "Specifically, MMClassification is based on `mmcv.EpochBasedRunner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980a88f-8670-438a-ac35-75267d2a41c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PyTorch codes under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f7182c-ed27-4320-997a-beace65f564f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EpochBasedRunner(BaseRunner):\n",
    "    \"\"\"Epoch-based Runner.\n",
    "    This runner train models epoch by epoch.\n",
    "    \"\"\"\n",
    "\n",
    "    def run_iter(self, data_batch: Any, train_mode: bool, **kwargs) -> None:\n",
    "        if self.batch_processor is not None:\n",
    "            outputs = self.batch_processor(\n",
    "                self.model, data_batch, train_mode=train_mode, **kwargs)\n",
    "        elif train_mode:\n",
    "            outputs = self.model.train_step(data_batch, self.optimizer,\n",
    "                                            **kwargs)\n",
    "        else:\n",
    "            outputs = self.model.val_step(data_batch, self.optimizer, **kwargs)\n",
    "        if not isinstance(outputs, dict):\n",
    "            raise TypeError('\"batch_processor()\" or \"model.train_step()\"'\n",
    "                            'and \"model.val_step()\" must return a dict')\n",
    "        if 'log_vars' in outputs:\n",
    "            self.log_buffer.update(outputs['log_vars'], outputs['num_samples'])\n",
    "        self.outputs = outputs\n",
    "\n",
    "    def train(self, data_loader, **kwargs):\n",
    "        self.model.train()\n",
    "        self.mode = 'train'\n",
    "        self.data_loader = data_loader\n",
    "        self._max_iters = self._max_epochs * len(self.data_loader)\n",
    "        self.call_hook('before_train_epoch')\n",
    "        time.sleep(2)  # Prevent possible deadlock during epoch transition\n",
    "        for i, data_batch in enumerate(self.data_loader):\n",
    "            self.data_batch = data_batch\n",
    "            self._inner_iter = i\n",
    "            self.call_hook('before_train_iter')\n",
    "            self.run_iter(data_batch, train_mode=True, **kwargs)\n",
    "            self.call_hook('after_train_iter')\n",
    "            del self.data_batch\n",
    "            self._iter += 1\n",
    "\n",
    "        self.call_hook('after_train_epoch')\n",
    "        self._epoch += 1\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def val(self, data_loader, **kwargs):\n",
    "        self.model.eval()\n",
    "        self.mode = 'val'\n",
    "        self.data_loader = data_loader\n",
    "        self.call_hook('before_val_epoch')\n",
    "        time.sleep(2)  # Prevent possible deadlock during epoch transition\n",
    "        for i, data_batch in enumerate(self.data_loader):\n",
    "            self.data_batch = data_batch\n",
    "            self._inner_iter = i\n",
    "            self.call_hook('before_val_iter')\n",
    "            self.run_iter(data_batch, train_mode=False)\n",
    "            self.call_hook('after_val_iter')\n",
    "            del self.data_batch\n",
    "        self.call_hook('after_val_epoch')\n",
    "\n",
    "    def run(self,\n",
    "            data_loaders: List[DataLoader],\n",
    "            workflow: List[Tuple[str, int]],\n",
    "            max_epochs: Optional[int] = None,\n",
    "            **kwargs) -> None:\n",
    "        \"\"\"Start running.\n",
    "        Args:\n",
    "            data_loaders (list[:obj:`DataLoader`]): Dataloaders for training\n",
    "                and validation.\n",
    "            workflow (list[tuple]): A list of (phase, epochs) to specify the\n",
    "                running order and epochs. E.g, [('train', 2), ('val', 1)] means\n",
    "                running 2 epochs for training and 1 epoch for validation,\n",
    "                iteratively.\n",
    "        \"\"\"\n",
    "        assert isinstance(data_loaders, list)\n",
    "        assert mmcv.is_list_of(workflow, tuple)\n",
    "        assert len(data_loaders) == len(workflow)\n",
    "        if max_epochs is not None:\n",
    "            warnings.warn(\n",
    "                'setting max_epochs in run is deprecated, '\n",
    "                'please set max_epochs in runner_config', DeprecationWarning)\n",
    "            self._max_epochs = max_epochs\n",
    "\n",
    "        assert self._max_epochs is not None, (\n",
    "            'max_epochs must be specified during instantiation')\n",
    "\n",
    "        for i, flow in enumerate(workflow):\n",
    "            mode, epochs = flow\n",
    "            if mode == 'train':\n",
    "                self._max_iters = self._max_epochs * len(data_loaders[i])\n",
    "                break\n",
    "\n",
    "        work_dir = self.work_dir if self.work_dir is not None else 'NONE'\n",
    "        self.logger.info('Start running, host: %s, work_dir: %s',\n",
    "                         get_host_info(), work_dir)\n",
    "        self.logger.info('Hooks will be executed in the following order:\\n%s',\n",
    "                         self.get_hook_info())\n",
    "        self.logger.info('workflow: %s, max: %d epochs', workflow,\n",
    "                         self._max_epochs)\n",
    "        self.call_hook('before_run')\n",
    "\n",
    "        while self.epoch < self._max_epochs:\n",
    "            for i, flow in enumerate(workflow):\n",
    "                mode, epochs = flow\n",
    "                if isinstance(mode, str):  # self.train()\n",
    "                    if not hasattr(self, mode):\n",
    "                        raise ValueError(\n",
    "                            f'runner has no method named \"{mode}\" to run an '\n",
    "                            'epoch')\n",
    "                    epoch_runner = getattr(self, mode)\n",
    "                else:\n",
    "                    raise TypeError(\n",
    "                        'mode in workflow must be a str, but got {}'.format(\n",
    "                            type(mode)))\n",
    "\n",
    "                for _ in range(epochs):\n",
    "                    if mode == 'train' and self.epoch >= self._max_epochs:\n",
    "                        break\n",
    "                    epoch_runner(data_loaders[i], **kwargs)\n",
    "\n",
    "        time.sleep(1)  # wait for some hooks like loggers to finish\n",
    "        self.call_hook('after_run')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c428b61-f0dd-45f6-a7b4-23006edb3767",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
